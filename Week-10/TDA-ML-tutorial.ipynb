{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating Persistence in ML Pipelines\n",
    "#### Author - Siddharth Setlur\n",
    "In this tutorial, we're going to incorporate persistence features into a simple, interpretable ML model. The point here is that although we might lose some accuracy compared to state-of-the-art (SOTA) deep learning architectures like the ones we've seen like CNNs or DNNs, we gain interpretability and run time speed. We're going to be working with a random forest classifier, an extension to the decision tree architecture we say in the Knot theory tutorial. Along with computing persistence, the entire process takes under a minute (at least on my laptop which has just a cpu with 8GB RAM). \n",
    "Because using Gudhi with autodiff is tricky, we're going to use giotto - which is well integrated into scikit learn. We need to install giotto and a few other packages, so \n",
    "run \n",
    "\n",
    "```conda env create -f requirements.yml```\n",
    "\n",
    "and activate\n",
    "```conda activate tda-env-giotto```.\n",
    "\n",
    "If the coda solving takes too long or it doesn't work, just use the tda-env we created earlier and pip install the required package whenever an import error is thrown. Just remember to pip install in the tda-env, i.e. run ```conda activate tda-env``` in the terminal before pip install  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Shape classification\n",
    "\n",
    "Topological losses are most appropriate when the dataset that we're working with has a clear underlying shape that persistence can help detect. In this example, we're going to build a classifier that classifies a synthetic dataset comprised of 3D shapes. This notebook is based on the the [giotto-tda tutorial](https://github.com/giotto-ai/giotto-tda/blob/master/examples/classifying_shapes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions.generate_datasets import make_point_clouds\n",
    "import numpy as np\n",
    "#get the point clouds and their labels\n",
    "point_clouds_basic, labels_basic = make_point_clouds(n_samples_per_shape=10, n_points=20, noise=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is always to examine the dataset we have. Pethaps, the first thing to do is to find the shape of the point clouds and the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_clouds_basic.shape, labels_basic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 30 labels, corresponding to the 30 different shapes we have. Each shape is a (400,3) array. But how many different labels are there, i.e. how many different kinds of shapes are we working with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a sample of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "samples_labels = []\n",
    "#get a single sample for each label\n",
    "for i in range(len(labels_basic)):\n",
    "    if labels_basic[i] not in samples_labels:\n",
    "        samples.append(#TODO )\n",
    "        samples_labels.append(#TODO)\n",
    "#Plot the point clouds on a 3D projection\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "# Create a figure with 3 subplots (one for each shape)\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "for i in range(#TODO):\n",
    "    ax = fig.add_subplot(1, 3, i+1, projection='3d')\n",
    "    ax.scatter(#TODO) #Hint - be careful  - you're plotting a 3D array!!\n",
    "    ax.set_title(f'Shape {int(samples_labels[i])}')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#save the point clouds and their labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the shapes? What do you expect their persistence diagrams to look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.plotting import plot_diagram\n",
    "homology_dimensions = [0, 1, 2]\n",
    "#Giotto has a very handy function to compute the persistence diagrams. Given a point cloud, it computes the VR complexes and then the persistence diagrams in the dimensions specified. \n",
    "VR_PD = VietorisRipsPersistence(homology_dimensions=homology_dimensions, collapse_edges=True) #this is a class that computes the diagrams given a point cloud, here we intiialize it to compute persistence in the 0,1,2 dimensions\n",
    "#compute the persistence diagrams for the point clouds\n",
    "#fit the persistence diagram\n",
    "pd1 = VR_PD.fit_transform(samples[0][None,:,:]) #circle\n",
    "pd2 = #TODO #sphere\n",
    "pd3 = #TODO #torus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VR class also comes witha nice plot function that plots the persistence diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VR_PD.plot(pd1) #diagram for the circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VR_PD.plot(#TODO) #diagram for the sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VR_PD.plot(#TODO) #diagram for the torus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the lecture, we need to compute vectorizations of the persistence diagrams in order to feed it into ML pipelines. Again Giotto makes our lives very easy by providing classes for a bunch of common representations. Here, we use the persistence landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.diagrams import PersistenceLandscape\n",
    "landscape = PersistenceLandscape()\n",
    "\n",
    "landscape_circ = landscape.fit_transform(pd1) #landscape for the circle\n",
    "landscape_sph = landscape.fit_transform(#TODO) #landscape for the sphere\n",
    "landscape_tor = #TODO #landscape for the torus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also super nice plotting functions for visualization! Can you see why a classifier fed the data of the persistence landscapes would be able to very easily classify the shapes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landscape.plot(landscape_circ) #landscape for the circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landscape.plot(#TODO) #landscape for the sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landscape.plot(#TODO) #landscape for the torus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train a classifier using just the landscapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "#split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(point_clouds_basic, labels_basic, test_size=0.2, random_state=42)\n",
    "#Compute the persistence diagrams for the training and test sets\n",
    "H_train = landscape.fit_transform(VR_PD.fit_transform(X_train))\n",
    "H_test = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLF = RandomForestClassifier(n_estimators=100, random_state=0, oob_score=True)\n",
    "#The issue is that we can't just feed 3 vectors in to the classifier, we can only feed scalars, so we sum along each of the landscapes, i.e. for each point cloud we have 3 landscapes which are 3 vectors each of legnth 100. We sum each of the vectors to get 3 numbers to feed into the classifier for each point cloud.\n",
    "CLF.fit(H_train.sum(axis=2), y_train)\n",
    "CLF.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just summing along the landscapes is very crude, but it works very well (actually it works perfectly), but we will soon see that this is not the case for real-world data and we'll have to get creative. Let's delve more into the statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Import necessary libraries for evaluation metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print out-of-bag score (accuracy)\n",
    "print(f\"Out-of-bag accuracy: {CLF.oob_score_:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(CLF.feature_importances_)), CLF.feature_importances_)\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Feature Importance')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Get predictions\n",
    "y_pred = CLF.predict(H_test.sum(axis=2))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Circle\", \"Sphere\", \"Torus\"])\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Circle\", \"Sphere\", \"Torus\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is amazing! How can we interpret the classifier (look at the feature importance plot)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a more complicated dataset. We use a 3D dataset from a [Princeton comupter vision course](https://www.cs.princeton.edu/courses/archive/fall09/cos429/assignment3.html) comprised of 4 classes with 10 samples each i.e. 40 total clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from openml.datasets.functions import get_dataset\n",
    "import pandas as pd\n",
    "df = get_dataset('shapes').get_data(dataset_format='dataframe')[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like each row contains a point and the label telling us which point cloud it belongs to. Let's see what the labels are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize each of the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_sample = df.query('target == \"human_arms_out0\"')[[\"x\", \"y\", \"z\"]].values\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(human_sample[:, 0], human_sample[:, 1], human_sample[:, 2])\n",
    "ax.set_title('Human Point Cloud')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vase_sample = df.query('target == \"vase0\"')[[\"x\", \"y\", \"z\"]].values\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(#TODO\n",
    "ax.set_title('Vase Point Cloud')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chair_sample = df.query('target == \"dining_chair0\"')[[\"x\", \"y\", \"z\"]].values\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(#TODO)\n",
    "ax.set_title('Chair Point Cloud')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biplane_sample = df.query('target == \"biplane0\"')[[\"x\", \"y\", \"z\"]].values\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(#TODO)\n",
    "ax.set_title('Biplane Point Cloud')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a weird way to label things - they've labelled each point cloud uniquely, i.e. we have human_arms_out0,...,human_arms_out9 and similarly for the other 3 classes. Somehow, we need to make a labelling array as we had in the toy example above, i.e. a 1-d numpy array of length 40 where each entry is either 0,1,2, or 3 depending on whether which class it belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros(40) # array with 40 zeros\n",
    "labels[10:20] = 1 # label the samples 10-20 as 1 corresponding to the vase\n",
    "labels[20:30] = 2 # label the samples 20-30 as 2 corresponding to the chair\n",
    "labels[30:] = 3 # label the samples 30-40 as 3 corresponding to the biplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weird labelling method does make it easier to extract a list of point clouds though! We can iterate over the unique labels of the df, since each df label corresponds to a unique point cloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_clouds = np.asarray(\n",
    "    [\n",
    "        df.query(\"target == @shape\")[[\"x\", \"y\", \"z\"]].values\n",
    "        for shape in df[\"target\"].unique()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homology_dimensions = [0, 1, 2]\n",
    "VR_PD = VietorisRipsPersistence(homology_dimensions=homology_dimensions, collapse_edges=True)\n",
    "landscape = PersistenceLandscape()\n",
    "CLF = RandomForestClassifier(n_estimators=100, random_state=0, oob_score=True)\n",
    "#fit the persistence diagram\n",
    "#split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(point_clouds, labels, test_size=0.2, random_state=42)\n",
    "#fit the persistence diagram\n",
    "H_train = #TODO\n",
    "CLF.fit(#TODO, y_train) #Remember to sum! \n",
    "CLF.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_test = landscape.fit_transform(VR_PD.fit_transform(X_test))\n",
    "# Print out-of-bag score (accuracy)\n",
    "print(f\"Out-of-bag accuracy: {CLF.oob_score_:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(CLF.feature_importances_)), CLF.feature_importances_)\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Feature Importance')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Get predictions\n",
    "y_pred = CLF.predict(H_test.sum(axis=2))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Human\", \"Vase\", \"Chair\", \"Biplane\"])\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Human\", \"Vase\", \"Chair\", \"Biplane\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a few more features and see if we can improve our metrics, but our performance on such a small datatset using a simple classifier is already great! Look into changing/adding features using other vectorizations like PersistenceImage or Betti curves. See the [giotto documentation](https://giotto-ai.github.io/gtda-docs/latest/modules/diagrams.html#representations) for implementations of these features, (Hint - they work the same way as landscapes except fro a change in name). You could also look into [features](https://giotto-ai.github.io/gtda-docs/latest/modules/diagrams.html#features) like number of points in the diagram. Once you've decided on your feature, the pipeline is as follows \n",
    "\n",
    "(point_clouds, labels) -> (x_train, y_train) (x_test, y_test). \n",
    "\n",
    "Compute topological features - Landscape/Image(VR(x_train)) (or use the features like number of points)\n",
    "\n",
    "Do some thing like summing if you have multiple vectors as we did for the landscape, essentially you can feed as many scalars as you want into the classifier but not vectors. \n",
    "\n",
    "Train the classifier using clf.fit(train_features)\n",
    "\n",
    "Compute topological features on the test set\n",
    "\n",
    "Predict using the classfier \n",
    "\n",
    "Display summary stats - Hint - you can basically copy the last cell displaying the statistics to do the last 2 steps with minor modifications depending on your pipeline\n",
    "\n",
    "Finally interpret the classifier and discuss why you think the feature you chose improced the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
