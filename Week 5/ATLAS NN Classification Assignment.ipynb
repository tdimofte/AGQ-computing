{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exotic searches at ATLAS with NN Classification\n",
    "\n",
    "## Marking\n",
    "\n",
    "You will get marks for completeing the different tasks within this notebook:\n",
    "\n",
    "Any code expected for you to complete will contain `## FINISH ME` indicating the code isn't expected to run until you have completed it.\n",
    "\n",
    "\n",
    "| <p align='left'> Title                         | <p align='left'> Number of marks |\n",
    "| -------------------------------------  | --- |\n",
    "| <p align='left'> Workshop Exercise 1                     | <p align='left'> 2 |\n",
    "| <p align='left'> Workshop Exercise 2                     | <p align='left'> 2 |\n",
    "| <p align='left'> Workshop Exercise 3                                   | <p align='left'> 1 |\n",
    "| <p align='left'> Assessment Exercise 1                                     | <p align='left'> 2 |\n",
    "| <p align='left'> Assessment Exercise 2                               | <p align='left'> 2 |\n",
    "| <p align='left'> Assessment Exercise 3                       | <p align='left'> 1 |\n",
    "| <p align='left'> **Total** | <p align='left'> max **10** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#  The next command should not be necessary, but may be used to force plots to appear inline in the notebook (if they're not showing up)\n",
    "#  %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workshop Exercise 1: Import, clean, and visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os    # for loading data from a local source\n",
    "\n",
    "foldername = 'Files/'\n",
    "bgd_files = ['Diboson.csv','Top.csv','Zjets.csv']\n",
    "sig_files = ['ggH1000.csv']\n",
    "\n",
    "bgd_df = []\n",
    "sig_df = []\n",
    "all_files = bgd_files + sig_files\n",
    "\n",
    "for index, file in enumerate(all_files):\n",
    "    size = os.path.getsize(foldername + file)/(1024*1024)\n",
    "    print ('Opening file',file,'with size',\"{:.1f}\".format(size),'MB:',)\n",
    "    tmp = pd.read_csv(foldername + file, index_col=0)      # reads csv files into a pandas DataFrame\n",
    "    if index < len(bgd_files):\n",
    "        bgd_df.append(tmp)\n",
    "    else:\n",
    "        sig_df.append(tmp)\n",
    "    print ('Done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: check the first few rows and columns in the data\n",
    "print(sig_df[0].iloc[:5,:12])\n",
    "print(bgd_df[0].iloc[:5,:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: check if cleaning is required to remove empty (NA) values -- and clean\n",
    "print (\"# of entries before cleaning S:\\n\", sig_df[0].count())\n",
    "print (\"# of entries before cleaning B0:\\n\", bgd_df[0].count())\n",
    "print (\"# of entries before cleaning B1:\\n\", bgd_df[1].count())\n",
    "print (\"# of entries before cleaning B2:\\n\", bgd_df[2].count())\n",
    "sig_df[0].dropna(inplace = True)\n",
    "    ## FINISH ME\n",
    "    ## FINISH ME\n",
    "    ## FINISH ME\n",
    "print (\"# of entries after cleaning S:\\n\", sig_df[0].count())\n",
    "print (\"# of entries after cleaning B0:\\n\", bgd_df[0].count())\n",
    "print (\"# of entries after cleaning B1:\\n\", bgd_df[1].count())\n",
    "print (\"# of entries after cleaning B2:\\n\", bgd_df[2].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "features = ['lep1_pt', ## FINISH ME using the features from Table 1, excluding the Boolean flag isSignal\n",
    "\n",
    "Nbins = 100\n",
    "for var in features:\n",
    "    #adopt a common binning scheme for all channels\n",
    "    bins_ = np.linspace(min(sig_df[0][var]), max(sig_df[0][var]), Nbins)\n",
    "    \n",
    "    plt.hist(bgd_df[0][var], histtype='step', density=True, bins=bins_, label='Dibosons', linewidth=2)\n",
    "    plt.hist(bgd_df[1][var], ## FINISH ME\n",
    "       ## FINISH ME: get three background datasets and the one signal dataset all in a single plot, by calling plt.hist four times\n",
    "    \n",
    "    \n",
    "    plt.xlabel(var)\n",
    "    plt.yscale('log')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workshop Exercise 2: Create the dataset for the classifier; plot correlations in features to see some differences between signal and background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ['isSignal']\n",
    "wtype = ['Background', 'Signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together all background samples to produce one mega-DataFrame\n",
    "totalBgd_df = pd.concat(bgd_df, ignore_index = True)\n",
    "print (\"total # of bgd events =\",totalBgd_df.shape[0])\n",
    "print (\"total # of sig events =\",sig_df[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomise the background samples\n",
    "# previously, we randomised (shuffled) the data while constructing pytorch dataloaders; this is a hands-on alternative\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def Randomise(df, random_seed):\n",
    "    df = shuffle(df, random_state=random_seed)\n",
    "    df = df.reset_index(drop=True) # do not insert a new column with the new index\n",
    "    return df\n",
    "\n",
    "Answer_to_all_questions = 42            # random seed for reproducibility\n",
    "print(totalBgd_df.iloc[:5,:12])\n",
    "totalBgd_df = Randomise(totalBgd_df, Answer_to_all_questions)\n",
    "print(totalBgd_df.iloc[:5,:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataset with equal #s of signal and background events\n",
    "Nsig = sig_df[0].shape[0]\n",
    "sigbgd_tmp = [totalBgd_df[0: ## FINISH ME], sig_df[0]]\n",
    "# merge these two dataframes into one\n",
    "sigbgd = pd.concat(  ## FINISH ME\n",
    "# randomise the new sample with equal #s of signal and background\n",
    "sigbgd = Randomise(  ## FINISH ME\n",
    "# check out the new dataframe\n",
    "print(sigbgd.head(5))\n",
    "print (\"total # of events =\",sigbgd.shape[0])\n",
    "print (\"# of signal events in new DF =\",len(sigbgd[sigbgd.isSignal == 1]))\n",
    "print (\"# of background events in new DF =\",  ## FINISH ME )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['lep1_pt', ## FINISH ME using just the first eight features from Table 1\n",
    "\n",
    "# reduce to desired features + output\n",
    "dataset = sigbgd[features + output]\n",
    "print (dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[features].values\n",
    "target = dataset[output].values\n",
    "print (data.shape, target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dt_utils import featureplot\n",
    "N_classes = 2\n",
    "featureplot(data, target, N_classes, t_names = features, c_names = wtype)\n",
    "\n",
    "# if you have 9 features, this will create (9 choose 2) = 9!/(2! 7!) = 36 plots. \n",
    "# if you have 8 features, this will create (8 choose 2) = 8!/(2! 6!) = 28 plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workshop Exercise 3: Rescale data and convert to PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling: we will now standardise the inputs (=scale their ranges so that they are roughly the same)\n",
    "# recall that, previously, we used torchvision transforms while loading datasets to do something similar; this is an alternative\n",
    "from sklearn import model_selection, preprocessing\n",
    "sc = preprocessing.StandardScaler()\n",
    "data = sc.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_to_all_questions = 42\n",
    "\n",
    "# train-test split of dataset and convert to pytorch tensors\n",
    "train_data, test_data, train_target, test_target = model_selection.train_test_split(\n",
    "    data, target, test_size=0.3, random_state=Answer_to_all_questions)\n",
    "\n",
    "print(train_data.shape, train_target.shape, test_data.shape, test_target.shape)\n",
    "\n",
    "import torch\n",
    "\n",
    "def xNumpyToTensor(array):\n",
    "    array = np.array(array, dtype=np.float32) \n",
    "    return torch.from_numpy(array).type(torch.FloatTensor)\n",
    "\n",
    "def yNumpyToTensor(array):\n",
    "    array = np.array(array.astype(int))\n",
    "    return torch.from_numpy(array).type(torch.FloatTensor)\n",
    "\n",
    "train_data_tensor = xNumpyToTensor(train_data)\n",
    "train_target_tensor = ## FINISH ME\n",
    "test_data_tensor = ## FINISH ME\n",
    "test_target_tensor = ## FINISH ME\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment Exercise 1: construct and train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epochs = 200\n",
    "num_nodes = 40\n",
    "num_inputs = 8    # num of inputs = 8 or 9\n",
    "num_outputs = 1   # num of outputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(p=0.2)     # see https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
    "\n",
    "# Define a PyTorch NN with just one hidden layer of width num_nodes\n",
    "# so num_inputs -> num_nodes -> num_nodes -> num_outputs\n",
    "# with activation function ReLu and dropout after the first two transformations, and a final Sigmoid activation function\n",
    "# altogether:  8 (or 9) -> 40, ReLu, dropout -> 40, ReLu, dropout -> 1, Sigmoid\n",
    "# (Think: why a final Sigmoid function?)\n",
    "\n",
    "def my_model(num_inputs, num_nodes):\n",
    "    model = nn.Sequential(\n",
    "\t\tnn.Linear( ## FINISH ME ), nn.ReLU(), dropout,\n",
    "\t\tnn.Linear( ## FINISH ME ), ## FINISH ME\n",
    "\t\tnn.Linear( ## FINISH ME ), ## FINISH ME\n",
    "\t)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_model(num_inputs, num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train the model\n",
    "loss_fn   = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "all_losses = []\n",
    "all_valid_accuracies = []\n",
    "\n",
    "#Training in batches\n",
    "for step in range(N_epochs):    \n",
    "    model.train()\n",
    "    out = model(train_data_tensor)                 # input x and predict based on x\n",
    "    cost = loss_fn(out,   ## FINISH ME\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    cost.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients \n",
    "\n",
    "    step_size = 10\n",
    "    \n",
    "    loss = cost.item()\n",
    "    all_losses.append(loss)\n",
    "    if step % step_size == 0:        \n",
    "        print(step, cost.data.cpu().numpy())\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        prediction = model(test_data_tensor)  # probabilities\n",
    "        predicted_labels = (prediction > 0.5).float()  # Convert probabilities to 0 or 1\n",
    "        actual_labels = test_target_tensor\n",
    "        accuracy = (predicted_labels.eq(actual_labels).sum() / float(actual_labels.nelement())).item()  # Calculate the accuracy\n",
    "        all_valid_accuracies.append(accuracy)\n",
    "        if step % step_size == 0:        \n",
    "            print('Validation accuracy: {:.1f}%'.format(accuracy * 100))\n",
    "        \n",
    "    # RuntimeError: can't convert CUDA tensor to numpy (it doesn't support GPU arrays). \n",
    "    # Use .cpu() to move the tensor to host memory first.        \n",
    "    ####prediction = (model(test_data_tensor).data).float() # probabilities                  \n",
    "    pred_y = prediction.cpu().numpy().squeeze()\n",
    "    target_y = test_target_tensor.cpu().data.numpy()\n",
    "    if step % step_size == 0:        \n",
    "        print ('LOG_LOSS={} '.format(log_loss(target_y, pred_y))) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now plot the losses (all_losses) and the accuracies (all_valid_accuracies) across all training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  FINISH ME\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment Exercise 2: Improve the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!   ## FINISH ME\n",
    "# Try to get better than 95% accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment Exercise 3: Visualise what's happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a confusion matrix\n",
    "\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix( ## FINISH ME )\n",
    "nn_utils.heatmap(cm, labels=['Predicted', 'True'], \n",
    "        classes=[wtype,wtype],\n",
    "        normalize=True)\n",
    "#sns.heatmap(cm, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "# Get 'Receiver operating characteristic' (ROC)\n",
    "fpr, tpr, thresholds = roc_curve(test_target, pred_y)\n",
    "\n",
    "# Compute \"Area Under the Curve\" (AUC) from prediction scores\n",
    "roc_auc  = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, lw=2, label='Area under curve: %0.2f)' % roc_auc)\n",
    "plt.plot([0, 0], [1, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.ylabel('True Positive Rate (Signal efficiency)')\n",
    "plt.xlabel('False Positive Rate (Background efficiency)')\n",
    "plt.title('ROC curve: Higgs signal vs. SM background')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agq-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
